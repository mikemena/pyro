1. Model Architecture & Training

[] Add train_model_v3 --> previous models are quite basic - just a simple feedforward network with ReLU and dropout. v3 adds the following:

- Batch normalization to stabilize training
- Learning rate scheduling (learning rate stays constant)
- Limited regularization techniques

2. Model Evaluation

[] Create evaluate.py with enhanced evaluation

[] Move the evaluate method from train_model_v3 to this new evaluate.py

3. Understand these components - what are they, when to use it, when not to use it:

## Batch Normalization (BN)
Batch Normalization (BN) is a technique used in AI modelingâ€”especially deep neural networksâ€”to stabilize and speed up training by normalizing the inputs to each layer.

When to Use Batch Normalization (BN)
âœ… Use BN:
âœ… Large batch sizes
âœ… CNNs or deep MLPs
âœ… Stable training speed desired

When to Avoid It
ðŸš« Batch size < 8â€“16
ðŸš« RNNs/sequence models (use Layer Norm instead)
ðŸš« Highly variable input distributions per batch

## Learning Rate Scheduling
Learning Rate Scheduling is a technique in AI modeling where the learning rateâ€”the size of each step the optimizer takes during trainingâ€”is changed over time instead of staying constant.

Think of the learning rate like the stride of a hiker on a mountain:

Early in the hike (training), you want big steps to cover ground quickly.

Near the peak (convergence), you want small steps to avoid overshooting.

A learning rate schedule automates that adjustment.

-- CosineAnnealingLR : Learning rate follows a cosine curve, starting   high and gradually reaching near zero

-- StepLR : Reduce the learning rate by a factor every fixed number of epochs.

-- ReduceLROnPlateau : Monitor validation loss; when it stops improving, lower the learning rate.

When to Use Learning Rate Scheduling
âœ… Long training runs â€” scheduling helps avoid wasting time at a suboptimal rate.
âœ… Deep networks â€” especially CNNs, Transformers, or deep MLPs.
âœ… When loss plateaus â€” good sign the LR might be too high to fine-tune further.
âœ… Transfer learning / fine-tuning â€” often start with lower LR and decay further.
âœ… When experimenting with optimizers â€” especially SGD, which benefits heavily from LR decay.

When to Avoid It
ðŸš« Small datasets or very short training â€” the schedule may not have enough time to make a difference.
ðŸš« Already using an adaptive optimizer like Adam, AdamW, or RMSprop â€” they adjust effective learning rates internally (though combining with schedules can still help).
ðŸš« When the model is underfitting badly â€” lowering LR too early can freeze progress.
ðŸš« If your initial LR is already optimal â€” a bad schedule can harm training instead of helping.

## Gradient clipping

Gradient Clipping is a technique used in AI modeling to prevent gradients from becoming too large during backpropagation.

When to Use Gradient Clipping
âœ… Recurrent Neural Networks (RNNs, LSTMs, GRUs) â€” they are especially prone to exploding gradients because of long-term dependencies.
âœ… Very deep networks â€” as depth increases, gradients can accumulate and blow up.
âœ… High learning rates â€” which can amplify unstable gradient updates.
âœ… When loss suddenly spikes â€” often a sign gradients are exploding.
âœ… Training GANs â€” where generator/discriminator competition can cause unstable updates.

When to Avoid It
ðŸš« If youâ€™re not experiencing exploding gradients â€” clipping might unnecessarily slow learning.
ðŸš« When the problem is vanishing gradients â€” clipping wonâ€™t help; you need different architectures or activation functions.
ðŸš« With well-tuned learning rates and batch normalization â€” sometimes clipping becomes redundant.
ðŸš« If the threshold is set too low â€” gradients will be constantly clipped, leading to underfitting.

## Regularization (ReLU, LeakyReLU, GELU)
Regularization in AI modeling is any technique that reduces overfitting by controlling model complexity or improving generalization.
It doesnâ€™t always mean â€œmaking the model smallerâ€ â€” sometimes it means â€œforcing the model to learn in a more robust way.â€

ReLU (Rectified Linear Unit)

- Zeroes out negative values, keeps positives unchanged.
- Introduces non-linearity so networks can model complex relationships.
- Simple, efficient, and helps avoid the vanishing gradient problem.

When to use ReLU (Rectified Linear Unit)
âœ… Training deep feedforward or convolutional networks.
âœ… You want fast convergence with stable gradients.

When to Avoid It
ðŸš« Inputs have a lot of negative values that still carry useful information â€” ReLU might â€œkillâ€ too many neurons (â€œdying ReLUâ€ problem).
ðŸš« Instead, try LeakyReLU or GELU.

LeakyReLU

- Like ReLU, but negative values are scaled by a small slope (
ð›¼, e.g., 0.01) instead of being zeroed.
- Prevents neurons from dying completely.

When to use LeakyReLU
âœ… Youâ€™re seeing dead neurons with plain ReLU.
âœ… Tasks where negative activations still carry meaningful patterns.

When to Avoid It
ðŸš« The added computation cost outweighs benefits (rarely a big issue).
ðŸš« You're working with activations that should be strictly non-negative.

GELU (Gaussian Error Linear Unit)
- Smooth, probabilistic version of ReLU.
- Retains small negative values in a smooth way
- Popular in Transformer architectures (e.g., BERT).

When to use GELU
âœ… You want smoother gradients than ReLU.
âœ… Using Transformer-like models or NLP tasks.

When to Avoid It
ðŸš« Training speed is critical (GELU is slower than ReLU).
ðŸš« You donâ€™t see noticeable performance improvement over ReLU.

## Weight Decay
Weight Decay is a form of regularization in AI modeling that discourages the modelâ€™s weights from growing too large.
It works by adding a penalty term to the loss function so that, during training, the optimizer favors smaller weights.

When to Use Weight Decay
âœ… Overfitting is an issue â€” especially with high-capacity models like large MLPs, CNNs, or Transformers.
âœ… When using SGD â€” SGD benefits a lot from weight decay to stabilize training.
âœ… For image and text models â€” itâ€™s a common default in vision and NLP.
âœ… With AdamW optimizer â€” AdamW decouples weight decay from the gradient step, making it more effective.

When to Avoid Weight Decay
ðŸš« When underfitting â€” shrinking weights further can make it harder for the model to learn.
ðŸš« If you already have strong regularization â€” heavy dropout, strong data augmentation, or early stopping might make weight decay redundant.
ðŸš« For bias terms or batch norm parameters â€” penalizing these can hurt performance (common practice is to exclude them from weight decay).
ðŸš« In sparse models where L1 regularization is preferred â€” e.g., when you want many weights to become exactly zero.

## dropout_rate

high dropout rate, which can be effective for preventing overfitting but may lead to underfitting if the model is too small or the data is limited. Since youâ€™re using a single hidden layer with 128 units, this high dropout rate might be aggressive. You could experiment with lower values e.g., 0.3 or 0.5

## hidden dims

## grid search or random search
